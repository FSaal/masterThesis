\chapter{Results}
\label{ch:Results}
% Do not write long version of imu or lidar acronym again
\glslocalunset{imu}
\glslocalunset{lidar}
\glslocalunset{ransac}
\glslocalunset{roi}
\glslocalunset{ros}
\glslocalunset{slam}

In this chapter the results of the different proposed methods will be presented.
At first, the collection of the reference data will be explained.
The different metrics used for the evaluation of the results will be described in \cref{sec:performance_measures}.
The different methods using the \gls{imu} are analyzed in \cref{sec:eval_imu}.
Afterwards the performance of the \gls{lidar} algorithm will be discussed in \cref{sec:eval_lidar} and finally the results of the object detection using the camera will be presented in \cref{sec:eval_camera}.



\section{Evaluation Concept}
During each test drive the measurements of the accelerometer and gyroscope of the \gls{imu} (of both \glspl{imu}, myAHRS+ and of the \gls{imu} integrated in the ZED 2i camera), the camera image of the ZED 2i camera and the point cloud generated by the \gls{lidar} are recorded.
Only one \gls{lidar} could be mounted at a time, so the Velodyne UltraPuck was used for most test drives, but two recordings were also made using the Robosense.
Furthermore, the wheel speeds were recorded when available, which was only the case when driving a ramp down or only half-way up, as mentioned in \cref{sec:car}.\par
To prevent an overfitting of the model it is important to have different test scenarios.
As mentioned in \cref{sec:garage}, four different ramps were available to test the model.
For the evaluation of the \gls{imu} only the ramps A B, and D will be used.
Multiple test drives were made for each scenario, the most drives where performed on ramp A.
Because the wheel speed sensor measurements are needed for some methods, which are only available in a special mode where the power of the car is very limited, the ramps A and B could only be driven halfway up.
But a full recording including the wheel speed measurements could be done for ramp D and A, when driving down.\par
The algorithms using the \gls{lidar} and camera only detect ramps going up, so only the ramps A, B and C were used.
Furthermore, a test drive without any ramps in sight was made to test if false positives are being detected.



\section{Reference Data}
To evaluate the performance of the different algorithms a reference is necessary.
The open-source \gls{ros} package \texttt{hdl\_graph\_slam}~\footnote{\url{https://github.com/koide3/hdl_graph_slam}}~\cite{Koide2019} is used for this task.
It is based on 3D graph \gls{slam} and uses the \gls{lidar} data to map the environment and estimate the pose (position and orientation) of the car.
It uses \gls{ndt} scan matching-based odometry estimation with loop detection.
The point cloud of common features at time $t$ is compared to the point cloud from the time $t-1$ and matched against each other.
The algorithm then estimates the translation and orientation difference between those two point clouds.
In \cite{Akpnar2021} the accuracy of the HDL Graph \gls{slam} was tested and a mean error of \SI{4}{\cm} and a standard deviation of \SI{5}{\cm} was measured for an indoor scenario.\par
From the pose information of the HDL Graph \gls{slam} the pitch angle of the car can be calculated and be used as a reference for the road grade, to evaluate the performance of the different \gls{imu}-based methods.
Because the \gls{lidar} only records at \SI{10}{\hertz} and thus the estimation of the HDL Graph \gls{slam} also only updates at a rate of \SI{10}{\hertz}, but the other sensors record from \SIrange{100}{400}{\hertz}, the estimate was upsampled using a Fourier method.
Besides estimating the road grade, the \gls{imu} is used to estimate the angle and length of the ramp.
The reference value for the length can be extracted from the generated point cloud map, by measuring the distance between the corresponding points.
Because the ramp angle is not constant, the average angle of the ramp is used as reference.
The average angle \gls{ramp_ang} is calculated by measuring the length and height of the ramp and using the law of sines
\begin{equation}
    \gls{ramp_ang} = \arcsin\left(\frac{h_\mathrm{ramp}}{l_\mathrm{ramp}}\right).
\end{equation}
The \gls{lidar} is used to detect and track the distance to the ramp and also to estimate the angle, width and length.
For the evaluation of the tracking accuracy, the generated map and pose provided by the HDL Graph \gls{slam} is used again.
In the generated point cloud map, the ramp region was marked manually by visual inspection.
Then, using the position of the car, provided by the \gls{slam}, the true distance to the beginning of the ramp could be calculated, by measuring the distance of the current position to the beginning of the ramp for each frame.\par
An example of the point cloud map generated by the \texttt{hdl\_graph\_slam} package is shown in \cref{fig:pcd_plotly}.
The color of the points gives information about the z-value (height information) of the points.
The ramp region is marked by the greater points in rainbow color and the black squares visualize the trajectory of the car.
In this example the car was driven only half-way up the ramp.
As mentioned in \cref{sec:methods_camera}, the camera is also used to identify the ramp.
Because manual labeling of all the images was necessary for the training of the network, the labels can be used as a reference.
\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth, trim={0 0 0 2cm}, clip]{pcd_plotly}
    \caption[Generated point cloud map]{The point cloud map generated by the \texttt{hdl\_graph\_slam} package. The ramp region was selected manually by visual inspection and is marked by the rainbow-colored points. The black squares visualize the trajectory of the car.}
    \label{fig:pcd_plotly}
\end{figure}



\section{Performance Measures}
\label{sec:performance_measures}
Using the \gls{imu}, the pitch angle of the car is estimated over time.
The goodness of the fit between the estimation $\hat{y} = (\hat{y}_i, \dots, \hat{y}_n)^\intercal$ and the reference $y = (y_i, \dots, y_n)^\intercal$ can then be described by the \gls{rmse}
\begin{equation}
    RMSE = \sqrt{\frac{1}{n}\sum_{i = 1}^n(\hat{y}_i - y_i)^2},
\end{equation}
which quantifies by how much the predicted values differ from the reference value on average.
It is defined in the range $[0, \infty)$, with a value of 0 indicating a perfect fit.\par
The same can be applied to the estimation for the angle, width, length and distance to the ramp by the \gls{lidar}-based method.
The only difference is that the reference angle, width and length are constant and thus the \gls{rmse} is basically the same as the standard deviation in this case.
Furthermore, the coefficient of determination $R^2$ is used to evaluate the pitch angle estimation
\begin{equation}
    R^2 = 1 - \frac{\sum\limits_{i = 1}^n(\hat{y}_i - y_i)^2}{\sum\limits_{i = 1}^n(\hat{y}_i - \overline{y})^2},
\end{equation}
where $\overline{y}$ indicates the mean of the reference.
The goodness of the fit is described in the range from 0 to 1, where 1 describes a perfect fit.\par
The \gls{lidar} is used to detect a ramp, and to calculate its properties if a ramp is detected.
The detection rate is evaluated by using the number of \glspl{tp} and \glspl{fn}.
What is counted as \gls{tp} or \gls{fn} always depends on the use case.
In the \gls{lidar} scenario, a frame is labeled as \gls{tp} if the ramp is visible to the \gls{lidar}, the algorithm detected a ramp and at least 70\% of the detected points actually lie inside the ramp region.
Analogously, a frame is classified as \gls{fn}, if a ramp is visible but less than 70\% of the detected points lie inside the ramp region.
Furthermore, when a ramp was detected even though none was visible, the frame is labeled as \gls{fp}.\par
The performance of the neural network can be evaluated in several ways.
One commonly used metric in the field of computer vision is the \gls{ap}.
It is a detection evaluation metric, commonly used by the \gls{coco} dataset.
It is based on the precision and recall score, which can be calculated by
\begin{equation}
    \text{Precision} = \frac{TP}{TP+FP}
\end{equation}
\begin{equation}
    \text{Recall} = \frac{TP}{TP+FN}.
\end{equation}
The precision gives information about how accurate the predictions are and the recall gives information about how much of the ground truth is detected.
The area under the precision-recall curve is the \gls{ap} score.
So, unlike the name might suggest, the \gls{ap} score is not actually just the average of the precision score.
Otherwise, a model which correctly detects some objects in the image, but misses others, would still achieve a score of 1.
Similarly to the \gls{lidar}, it first must be defined what classifies as a \gls{tp} and a \gls{fp}.
For this, a new metric is introduced, the \gls{iou}, which is defined as
\begin{equation}
    \text{\acrshort{iou}} = \frac{\text{Area of Overlap}}{\text{Area of Union}}.
\end{equation}
The area of overlap is defined as the intersection between the predicted and ground truth bounding box, and the area of union is the area of both boxes added together.
Instead of a bounding box, a segmentation mask can be used as well.
A perfect detection would have an \gls{iou} score of 1.
Because an \gls{iou} score of 1 is nearly impossible, a frame is labeled as \gls{tp} instead, if the \gls{iou} score is greater than a certain threshold $m$.
The score is then written as $\text{AP}_{m}$.
For example, $\text{AP}_{50}$ means how many detections have been made with an \gls{iou} score of at least 50\%.
Since selecting a meaningful threshold for the \gls{ap} score depends on the dataset, the \gls{map} is often used instead.
The \gls{map} score is the average over multiple \gls{iou} thresholds for all the different classes.
It is calculated by averaging all \gls{ap} scores in the range of 50\% to 95\% with a step size of 5\%, meaning that the average of 10 different values is used.



\section{\glsentryshort{imu}-based measurements}
\label{sec:eval_imu}
In this section the different methods to estimate the road grade using the \gls{imu} are being evaluated.
The results for two different drives are visualized.
Lastly, the estimated average ramp angle and length are being compared to the ground truth.

\subsection{Road Grade Estimation}
The ramp detection using the \gls{imu} relies on the correct estimation of the road grade angle.
Hence, the evaluation of the quality of the estimation is necessary to determine the performance of the ramp detection algorithm.\par
Different recordings of different ramps were made, but the results will be discussed on only two test drives.
Furthermore, two different \glspl{imu} were used, but only the measurements of one \gls{imu} will be used to present the results, since both produced similar results.
Because the \gls{imu} of the ZED camera is slightly more accurate as shown in \cref{tab:imu_datasheets}, only those measurements will be used.\par
In the first drive the car was accelerated from stand still and drove up ramp A about half-way up.
As explained in \cref{sec:car}, the ramp could not be driven completely, due to the need of the odometer readings, which are only available when the motor output is limited.
The result of using only the raw measurements of the \gls{imu} for the pitch angle calculation is illustrated in \cref{fig:imu_raw_angle}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.9\linewidth]{imu_raw_angle.pdf}
    \caption[\glsentryshort{imu} angle estimation using raw measurements]{Pitch angle estimation from the raw accelerometer and gyroscope measurements. The drive did start from stand still and did end in the middle of the ramp.}
    \label{fig:imu_raw_angle}
\end{figure}
The measurement by the accelerometer is very noisy and is easily influenced by accelerations other than gravity, which can be seen at time \SIrange{2}{4}{\second}, where the car started driving.
The gyroscope on the other hand provides good short-term accuracy and is not influenced by other accelerations, but is slowly drifting over time.
The reference is taken from the orientation estimation of the \texttt{hdl\_graph\_slam} package, which uses the \gls{lidar} data.
The time frame during which the car was on the ramp is marked by the yellow coloring.
The beginning of the ramp is classified as the point, where the reference data surpasses \ang{1.5}.\par
The gravity method tries to overcome the problem of the accelerometer of also detecting other accelerations than gravity, by subtracting the car's acceleration from the accelerometer measurement.
The car's acceleration $\vb{a}_\mathrm{odom,x} $ was calculated by calculating the derivate of the low-pass filtered car velocity $v_\mathrm{car} $, which was calculated from the wheel speed measurements.
\Cref{fig:imu_odometer_acc} shows the (low-pass filtered) acceleration measured by the \gls{imu} along the x-axis and the (low-pass filtered) car's acceleration.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.9\linewidth]{imu_odometer_acc.pdf}
    \caption[\glsentryshort{imu} and wheel speed sensors measured acceleration]{The measured acceleration in x-direction by the \glsentryshort{imu} and the acceleration derived from the wheel speed measurements and the difference between both.}
    \label{fig:imu_odometer_acc}
\end{figure}
And $\vb{a}_\mathrm{grav,x} $ is the acceleration measured by the \gls{imu} from which the car acceleration $\vb{a}_\mathrm{odom,x} $ was subtracted.
It can be seen, that especially at the beginning of the ramp (\SIrange{21}{23}{\second}) the gravity method shows it advantages.
The deceleration before entering the ramp is measured by both sensors and thus cancels out each other.
The same can be seen in the initial acceleration phase, where the car starts to drive from still stand (\SIrange[]{2}{4}{\second}).
Although both sensors are synchronized in time, the \gls{imu} senses the acceleration earlier than the wheel speed sensors which leads to a slight pike.
This could be due to the wheel speed sensors having a certain velocity threshold, below which they do not pick up any changes.
Other reasons for the difference could be, that other forces than the one from the car are present, e.g. from the suspension of the car, vibrations due to the road quality or movement in the car, which are not measured by the wheel speed sensors.
Moreover, the approximations made by calculating the finite difference of the car velocity to get the car acceleration have a negative influence on the result.\par
The resulting angles calculated from the accelerations can be seen in \cref{fig:imu_odometer_angle}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.9\linewidth]{imu_odometer_angle.pdf}
    \caption[\glsentryshort{imu} angle estimation using the gravity method]{Pitch angle estimation using only the accelerometer compared to the gravity method, which additionally uses the wheel speed measurements.}
    \label{fig:imu_odometer_angle}
\end{figure}
It can be seen that the gravity method improves the estimation accuracy, compared to when only the accelerometer data is used.
Especially the angle at the start and on the ramp is more accurately described when subtracting the odometer data from the accelerometer data for the calculation.
But it can also be seen that the time synchronization between both signals is important, which becomes apparent during the initial acceleration phase.
Here, the subtraction leads to a negative value, which neither sensor had measured.\par
Another way to improve the estimation is using a complementary filter.
The results of which, together with all other methods, are shown in \cref{fig:imu_all_angles}.
Those are the results of a new drive, where the car was driven the ramp D and afterwards ramp A down.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.9\linewidth]{imu_all_angles.pdf}
    \caption[\glsentryshort{imu} angle estimation comparison of all methods]{Comparison of different methods to estimate the pitch angle. Two different ramps were driven down successively.}
    \label{fig:imu_all_angles}
\end{figure}
The complementary filter uses the estimation of the gyroscope measurements and corrects them using the accelerometer measurements to prevent drift.
It can be seen that the estimation using the complementary filter closely follows the reference except for the part between the two ramps (\SIrange{30}{60}{\second}).
Also, it has no offset at the end, unlike the estimation from the gyroscope data.
The results of the other methods applied to the recordings of the same ride are also shown in the same figure.
Using the gyroscope measurements, the angle estimation follows the reference data very closely for the first ramp.
But at the end a drift can be observed.
The gravity method reduces the spikes of the raw accelerometer estimation, but introduces a new error at the beginning, due to the odometer readings being slightly shifted in time in regard to the accelerometer readings.\par
While a visual inspection gives a first impression on which method performs the best, the use of different metrics allows for a more accurate evaluation.
The \gls{rmse}, $R^2$ as well as the maximal error of the different methods when driving up, for two kinds of ramps, are shown in \cref{tab:eval_imu_up}.
Multiple recordings have been made for each ramp, but in all of them the car started from stand still and ended in the middle of the ramp.
The average of all drives was then calculated.\par
The estimation using the raw accelerometer data performs the worst and has the greatest maximal error.
Using the gyroscope the deviation from the reference data is fairly small, which can be seen in the maximal error.
But due to the drifting the errors cumulate, resulting in a high \gls{rmse} value.
The acceleration method performs well, but is negatively influenced by the acceleration from stand still, during which the accelerometer and wheel speed data are not properly aligned.
The best results are achieved using the complementary filter.
\begin{table}[htb]
    \centering
    \caption[\glsentryshort{imu} road grade estimation for uphill drives]{Road grade estimation when driving ramps up.}
    \label{tab:eval_imu_up}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
                                & \multicolumn{3}{c}{\textbf{Ramp A}} & \multicolumn{3}{c}{\textbf{Ramp B}}                                                                                                                                   \\
            \midrule
            \textbf{Method}     & \textbf{RMSE} [\si{\degree}]        & $\mathbf{R^2}$                      & $\mathbf{Error_{max}}$ [\si{\degree}] & \textbf{RMSE} [\si{\degree}] & $\mathbf{R^2}$   & $\mathbf{Error_{max}}$ [\si{\degree}] \\
            \cmidrule(lr){2-4}   \cmidrule(lr){5-7}
            Accelerometer       & 0.875                               & 0.675                               & 4.944                                 & 0.650                        & 0.787            & 3.378                                 \\
            Gyroscope           & 0.508                               & 0.905                               & 1.373                                 & 0.768                        & 0.802            & $\mathbf{1.594  }$                    \\
            Acceleration method & 0.357                               & 0.954                               & 2.482                                 & 0.351                        & $\mathbf{0.955}$ & 1.707                                 \\
            Complementary       & $\mathbf{0.273} $                   & $\mathbf{0.976}$                    & $\mathbf{1.231 }$                     & $\mathbf{0.324}$             & 0.952            & 1.647                                 \\
            % Complementary grav  & 0.371                               & 0.948                               & 1.990                  & 0.358            & 0.951            & 1.211                  \\
            \bottomrule
        \end{tabular}
    }
\end{table}
In \cref{tab:eval_imu_down} the average results for down drives are shown.
The data are from a test drive in which at first the ramp C was driven down, followed by ramp A in one consecutive drive.
The drive was performed multiple times and the average of all results was calculated again.
The estimations for one exemplary ride were illustrated in \cref{fig:imu_all_angles}.
The values for ramp C correspond to the measurements from \SIrange{0}{31}{\second} and the measurements from \SI{31}{\second} until the end are attributed to ramp A.
It can be seen that at the beginning (ramp C) the gyroscope performs the best, but with increasing time the drift negatively impacts the results.
All methods perform significantly worse in the second part of the drive (ramp A), due to the deviation of the reference value in the time frame between the two ramps.
But when looking at the graph in \cref{fig:imu_all_angles} it can be seen, that the gravity method and especially the complementary filter closely follows the reference data.
\begin{table}[htb]
    \centering
    \caption[\glsentryshort{imu} road grade estimation for downhill drives]{Road grade estimation when driving ramps down.}
    \label{tab:eval_imu_down}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
            \textbf{Method}     & \multicolumn{3}{c}{\textbf{Ramp C}} & \multicolumn{3}{c}{\textbf{Ramp A}}                                                                                                                                   \\
            \midrule
                                & \textbf{RMSE} [\si{\degree}]        & $\mathbf{R^2}$                      & $\mathbf{Error_{max}}$ [\si{\degree}] & \textbf{RMSE} [\si{\degree}] & $\mathbf{R^2}$   & $\mathbf{Error_{max}}$ [\si{\degree}] \\
            \cmidrule(lr){2-4}   \cmidrule(lr){5-7}
            Accelerometer       & 0.894                               & 0.746                               & 8.291                                 & 0.938                        & 0.784            & 3.967                                 \\
            Gyroscope           & $\mathbf{0.219}$                    & $\mathbf{0.990}$                    & $\mathbf{0.736}$                      & 0.784                        & 0.867            & $\mathbf{1.481}$                      \\
            Acceleration method & 0.369                               & 0.964                               & 2.211                                 & $\mathbf{0.719}$             & $\mathbf{0.872}$ & 2.136                                 \\
            Complementary       & 0.327                               & 0.977                               & 1.184                                 & 0.753                        & 0.862            & 2.131                                 \\
            \bottomrule
        \end{tabular}
    }
\end{table}


\subsection{Estimation of Ramp Properties}
Now that the estimation of the road grade is done, the ramp properties can be estimated.
The estimated ramp angles and the corresponding deviation from the reference value for three different ramps are listed in \cref{tab:eval_imu_angle}.
All the different methods seem to underestimate the angle when driving up (ramp A and B) or overestimate it when driving down (ramp D).
Because ramp A and B could only be driven half-way up, it could be possible that the average angle had not been reached yet.
Overall it can be seen that the complementary filter and the acceleration method perform the best.\par
\begin{table}[htb]
    \centering
    \caption[\glsentryshort{imu} ramp angle estimation]{Estimation of the average ramp angle.}
    \label{tab:eval_imu_angle}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
                                & \multicolumn{2}{c}{\textbf{Ramp A}} & \multicolumn{2}{c}{\textbf{Ramp B}} & \multicolumn{2}{c}{\textbf{Ramp D}}                                                                                                 \\
            \midrule
            \textbf{Method}     & \textbf{Angle} [\si{\degree}]       & \textbf{Error} [\si{\degree}]       & \textbf{Angle} [\si{\degree}]       & \textbf{Error} [\si{\degree}] & \textbf{Angle} [\si{\degree}] & \textbf{Error} [\si{\degree}] \\
            \cmidrule(lr){2-3}  \cmidrule(lr){4-5}   \cmidrule(lr){6-7}
            Accelerometer       & 4.14                                & 3.06                                & 4.75                                & 1.75                          & -10.41                        & 2.11                          \\
            Gyroscope           & 4.97                                & 2.23                                & 4.30                                & 2.20                          & -9.11                         & 0.81                          \\
            Acceleration method & 6.15                                & 1.05                                & 6.18                                & 0.32                          & -9.52                         & 1.22                          \\
            Complementary       & 5.42                                & 1.78                                & 6.12                                & 0.38                          & -9.01                         & 0.71                          \\
            \bottomrule
        \end{tabular}
    }
\end{table}
As discussed in \cref{ssec:ramp_detection_imu}, the length of the ramp can be estimated by integrating the velocity of the car over the time interval between the start and end of the ramp.
The velocity can be estimated in two different ways.
Either by using the wheel speed measurements, or by integrating the accelerometer measurements.
For the evaluation of the accuracy of the estimation, recordings of a drive with a full traverse of a ramp is necessary.
As mentioned in \cref{sec:car}, this is only possible when driving down.
Ramp D is used for this evaluation, the first part of the drive is show in \cref{fig:imu_all_angles}.\\
In \cref{fig:imu_distance_velocity} different methods to estimate the velocity of the car are shown.
\begin{figure}[htb]
    \centering
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{imu_distance_velocity.pdf}
        \caption[Car velocity estimation]{Estimated car velocity calculated from the wheel speed measurements and different methods using the accelerometer data.}
        \label{fig:imu_distance_velocity}
    \end{subfigure}

    % \bigskip
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{imu_distance_position.pdf}
        \caption[Car travelled distance estimation]{The integration of the velocity leads to the travelled distance. The length of the ramp can then be calculated from the difference between the position at the end and start of the ramp.}
        \label{fig:imu_distance_position}
    \end{subfigure}
    \caption[Ramp length estimation using various methods]{Comparison of different methods to estimate the length of the ramp.}
\end{figure}
$v_\mathrm{odom}$ is the velocity calculated from the wheel speed measurements using \cref{eq:v_car}.
The other three methods are based on the integration of the accelerometer measurements.
$v_\mathrm{acc}$ is the result of integrating the raw accelerometer measurements $\vb{a}_x$.
Because the raw signal also contains the gravity component, the estimation is not correct anymore when entering the ramp.
Up until the point where the car enters the ramp both estimations are very similar, but during the phase on the ramp the acceleration due to gravity disturbs the estimation.
This problem is partially solved by $v_\mathrm{acc, grav}$, which eliminates the gravity component by subtracting it using the pitch angle of the car, according to \cref{eq:acc_from_imu_wo_grav}.
Different angles can be used in the equation, the angle estimation using the complementary filter was chosen, since it achieved the best results.
Nonetheless, because the angle estimation of the angle is not correct at the start of the acceleration (\SIrange{3}{5}{\second}), an error is introduced.
As it can be seen in \cref{fig:imu_all_angles} the angle is slightly overestimated by the complementary filter during the acceleration phase.
This leads according to \cref{eq:acc_from_imu_wo_grav} to an underestimation of the acceleration, and thus also an underestimation of the velocity.
Due to the integration the error is cumulated over time and increases with the time.\par
This error can be fixed by introducing a new method, which ignores the angle estimation during the acceleration phase and instead assumes an angle of zero during this phase.
The result is visualized by $v_\mathrm{acc, grav, delayed}$.
For the sake of consistency, the reference data is once again calculated from the pose provided by the \texttt{hdl\_graph\_slam} package.
But it must be noted, that the data from the wheel sensors is most likely more accurate, but the results will still be compared to the reference data.\par
The estimation of the travelled distance for the different methods is shown in \cref{fig:imu_distance_position}.
Since the distance is calculated by integrating the velocity, an error is cumulated over time.
This can be seen in both $d_\mathrm{acc}$ and $d_\mathrm{acc, grav}$.
The only usable results are produced by the method using the odometer measurements and by $v_\mathrm{acc, grav, delayed}$.\par
The actual length of the ramp is calculated by subtracting the position of the car at the end of the ramp from the position at the start of the ramp.
The calculated length for the different methods and the corresponding error are shown in \cref{tab:ramp_length}.
\begin{table}[htb]
    \centering
    \caption[\glsentryshort{imu} ramp length estimation]{Estimation of the ramp length.}
    \label{tab:ramp_length}
    \begin{tabular}{lSS}
        \toprule
        \textbf{Method}                & {\textbf{Length} [\si{\metre}]} & {\textbf{Error} [\si{\metre}]} \\
        \midrule
        $d_\mathrm{odom} $             & 11.88                           & 0.63                           \\
        $d_\mathrm{acc} $              & -8.44                           & 20.69                          \\
        $d_\mathrm{acc, grav, } $      & 4.52                            & 7.73                           \\
        $d_\mathrm{acc, grav, later} $ & 11.85                           & 0.60                           \\
        $d_\mathrm{ref} $              & 11.25                           & 0                              \\
        \bottomrule
    \end{tabular}
\end{table}



\section{Ramp Detection by \glsentryshort{lidar}}
\label{sec:eval_lidar}
The \gls{lidar} is used to detect if a ramp is visible, track the position to the ramp as well as to estimate the angle, width and length of the ramp.
Due to the setup it was only possible to mount one \gls{lidar} at a time.
To get the best possible results the \gls{lidar} with the best resolution, especially in vertical direction, should be used.
A comparison of the resolution of the two \glspl{lidar} can be seen in \cref{fig:lidar_resolution_eval}.
\begin{figure}[htb]
    \centering
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{lidar_resolution_velodyne_eval.pdf}
        \caption{Velodyne}
        \label{fig:lidar_resolution_velodyne_eval}
    \end{subfigure}

    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{lidar_resolution_robos_eval.pdf}
        \caption{Robosense}
        \label{fig:lidar_resolution_robos_eval}
    \end{subfigure}
    \caption[Resolution comparison of the two \glsentryshortpl{lidar}]{Resolution comparison of the two \acrshortpl{lidar}. The ramp region is framed in black.}
    \label{fig:lidar_resolution_eval}
\end{figure}
Here, a top-down view on the ramp of the points measured by the \gls{lidar} at a distance of \SI{7}{\metre} to the ramp is shown.
The blue points visualize all the points which were visible to the ramp detection algorithm after the downsampling and pass through filter has been applied.
The points marked red symbolize the points that were then detected as part of the ramp by the algorithm.
The ramp region is framed in black and was drawn manually by visually inspecting the by the \texttt{hdl\_graph\_slam} package generated point cloud.
The Velodyne \gls{lidar}, shown in \cref{fig:lidar_resolution_velodyne_eval}, provides many more lines compared to the Robosense, shown in \cref{fig:lidar_resolution_robos_eval}.
Hence, only the results produced using the Velodyne \gls{lidar} will be discussed from here on.\par
The detection rate is evaluated by looking at the number of \glspl{tp} and \glspl{fn}, as described in \cref{sec:performance_measures}.
For all the other properties, the \gls{rmse} between the predicted values and the values measured in the point cloud, are calculated.
Since the resolution improves the closer the car gets to the ramp, the evaluation is divided into distance intervals of \SI{5}{\metre} length.\par
The results for three different ramps are shown in \cref{tab:eval_lidar}.
Each drive started about \SI{30}{\metre} from the ramp.
It can be seen that the detection works very well if the distance to the ramp is \SI{20}{\metre} or less.
For the ramp C it can be seen, that the detection is only reliable when the car is less than \SI{15}{\metre} away from the ramp.
This can be explained by the taken path during the recording, which was started at an offset in y-direction to the ramp.
Due to the passthrough filter the ramp was thus not visible to the algorithm, and could not be detected at the beginning.
It can also be observed, that the deviation of the width estimation is significantly smaller than the deviation of the distance and length estimation.
This can be explained by the working principle of the \gls{lidar}, which sends out laser points in horizontal lines, as depicted in \cref{fig:lidar_resolution_eval}.
The horizontal resolution is thus greater than the vertical resolution.
Furthermore, it must be noted that all calculations are done one the downsampled point cloud, which reduced the resolution to \SI{10}{\cm}.\par
Besides the three ramps, data consisting of 252 frames were recorded of an environment where no ramp was visible.
One frame was wrongly labeled as a ramp (\gls{fp}), which results together with the other recordings in a precision score of \SI{99.97}{\percent} and recall score of \SI{96.54}{\percent}.\par
\renewcommand{\arraystretch}{1}
\begin{table}[htb]
    \centering
    \caption[\glsentryshort{lidar} ramp detection evaluation]{Evaluation of the ramp detection and estimation of properties of the \glsentryshort{lidar} algorithm.}
    \label{tab:eval_lidar}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{ccccccccc}
            \toprule
            \textbf{Ramp}        & \textbf{Distance} [\si{\metre}] & \textbf{Frames}         & \textbf{TP}[\%]   & \textbf{FN}[\%]   & \multicolumn{4}{c}{\textbf{RMSE}}                       \\
            \cmidrule(lr){6-9}
            \multicolumn{4}{c}{} &                                 & $\theta [\si{\degree}]$ & $d [\si{\metre}]$ & $w [\si{\metre}]$ & $l [\si{\metre}]$                                       \\
            \midrule
            \multirow{6}{*}{A}   & \SIrange{0}{5}{}                & 116                     & 100.00            & 0.00              & 0.31                              & 0.70 & 0.03 & 1.27  \\
                                 & \SIrange{5}{10}{}               & 117                     & 100.00            & 0.00              & 0.30                              & 0.77 & 0.04 & 0.94  \\
                                 & \SIrange{10}{15}{}              & 116                     & 100.00            & 0.00              & 0.34                              & 0.81 & 0.07 & 1.03  \\
                                 & \SIrange{15}{20}{}              & 123                     & 100.00            & 0.00              & 0.27                              & 1.01 & 0.05 & 1.84  \\
                                 & \SIrange{20}{25}{}              & 140                     & 99.23             & 0.77              & 0.65                              & 1.70 & 0.12 & 6.28  \\
                                 & \SIrange{25}{30}{}              & 50                      & 59.78             & 40.22             & 1.79                              & 1.21 & 0.25 & 10.04 \\
            \addlinespace
            \multirow{6}{*}{B}   & \SIrange{0}{5}{}                & 62                      & 100.00            & 0.00              & 0.52                              & 0.71 & 0.02 & 0.81  \\
                                 & \SIrange{5}{10}{}               & 62                      & 100.00            & 0.00              & 0.53                              & 0.77 & 0.02 & 0.78  \\
                                 & \SIrange{10}{15}{}              & 59                      & 100.00            & 0.00              & 0.51                              & 0.86 & 0.02 & 0.85  \\
                                 & \SIrange{15}{20}{}              & 61                      & 97.92             & 2.08              & 0.49                              & 1.23 & 0.02 & 3.30  \\
                                 & \SIrange{20}{25}{}              & 61                      & 97.83             & 2.17              & 0.83                              & 2.18 & 0.12 & 9.33  \\
                                 & \SIrange{25}{30}{}              & 59                      & 42.75             & 57.25             & 1.82                              & 4.35 & 0.90 & 11.98 \\
            \addlinespace
            \multirow{6}{*}{C}   & \SIrange{0}{5}{}                & 21                      & 100.00            & 0.00              & 0.64                              & 0.87 & 0.05 & 2.58  \\
                                 & \SIrange{5}{10}{}               & 23                      & 100.00            & 0.00              & 0.50                              & 0.89 & 0.11 & 4.40  \\
                                 & \SIrange{10}{15}{}              & 28                      & 100.00            & 0.00              & 0.44                              & 0.70 & 0.10 & 1.23  \\
                                 & \SIrange{15}{20}{}              & 27                      & 37.04             & 62.96             & 0.66                              & 0.79 & 0.19 & 1.27  \\
                                 & \SIrange{20}{25}{}              & 29                      & 0.00              & 100.00            &                                   &      &      &       \\
                                 & \SIrange{25}{30}{}              & 28                      & 10.71             & 89.29             & 1.91                              & 1.40 & 0.25 & 10.78 \\
            \bottomrule
        \end{tabular}
    }
\end{table}
\renewcommand{\arraystretch}{1.2}
The estimated ramp properties and distance to the ramp for one exemplary ride are shown in \cref{fig:lidar_eval}.
All values are plotted against the distance to the ramp, which is almost linear to the time, since after the initial acceleration from stand still, the car moved at a constant speed.
In \cref{fig:lidar_distance_eval} the estimated distance from the car to the ramp is shown in comparison to the reference distance provided by the \texttt{hdl\_graph\_slam} package.
The error reduces when the car is closer to the ramp.
Interestingly the value of the estimated distance seems to hold itself for several \si{\metre}.
This is most probably due to the vertical resolution of the \gls{lidar}.
Because the vertical resolution is not linear, the most lines are centered around the middle of the opening angle of the \gls{lidar}, which is -\ang{5} for the Velodyne.
Hence, only few lines fall into the region at the start of the ramp.
The distance to the ramp is calculated by measuring the distance from the car to the $n$ closest points which have been identified as part of the ramp.
Therefore, the distance can only be updated if a line which has previously hit the ground now hits the ramp.\par
\Cref{fig:lidar_angle_eval} shows the difference between the estimated angle and the measured average angle.
It can be seen that the estimation varies by about \ang{1} if the distance is less than \SI{20}{\metre}.
The angle is almost exclusively underestimated, which could be due to the fact that the reference value measurement is not perfect.\par
The estimated width at different distances to the ramp can be seen in \cref{fig:lidar_width_eval}.
The error is very small compared to the tracking error and lies in the order of \SI{10}{\cm}.
This is because the horizontal resolution is significantly better than the vertical resolution.
A deviation of up to \SI{10}{\cm} is expected, since this is the resolution of the downsampled point cloud.
Note that the estimated width is the width of the whole ramp and not only the width of the drivable part.\par
Finally, the estimated length of the ramp is shown in \cref{fig:lidar_length_eval}.
At a large distance not enough lines fall into the ramp region, which leads to an underestimation of the length at first.
However, the estimation improves at a distance of about \SI{17}{\metre} to the ramp, after which the estimation fluctuates around the measured value.\par
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{lidar_distance_eval.pdf}
        \caption{Distance to the ramp}
        \label{fig:lidar_distance_eval}
    \end{subfigure}

    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{lidar_angle_eval.pdf}
        \caption{Angle of the ramp}
        \label{fig:lidar_angle_eval}
    \end{subfigure}

    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{lidar_width_eval.pdf}
        \caption{Width of the ramp}
        \label{fig:lidar_width_eval}
    \end{subfigure}

    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{lidar_length_eval.pdf}
        \caption{Length of the ramp}
        \label{fig:lidar_length_eval}
    \end{subfigure}
    \caption[\glsentryshort{lidar} estimation of ramp properties at different distances]{Estimated ramp properties and tracking of the distance to the ramp at different distances to the ramp.}
    \label{fig:lidar_eval}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{points_projection.pdf}
    \caption[2D visualization of the \glsentryshort{lidar} point cloud]{\acrshort{lidar} points projected onto the camera image. The green points were identified as part of a ramp by the algorithm.}
    \label{fig:points_projection}
\end{figure}
A visualization of the detection algorithm can be seen in \cref{fig:points_projection}.
Here, the 3D point cloud generated by the \gls{lidar} is projected onto the 2D camera image.
The quality of the projection depends on the accuracy of the measured translation and orientation difference between both sensors.
It can be seen that it is not perfect, e.g. the points do not quite match the camera image at the left pillar or the pipe on the ceiling.
Nonetheless, it gives a good indication of what the \gls{lidar} actually sees.
The coloring of the point indicates the distance from the car to the points.
Objects far away are marked by yellow points and nearby objects by blue points.
The green points were identified as part of the ramp by the algorithm.
It mostly fits the actual ramp very well.
The previously mentioned problem of the vertical resolution is clearly visible here.
While the density of the laser lines is sufficient in the ramp region, the start of the ramp and especially the ground is covered by very few lines.
This makes the precise tracking of the distance to the ramp difficult.



\section{Camera}
\label{sec:eval_camera}
\subsection{Object Recognition}
The camera was used to detect the ramp and mask the corresponding region in the image.
As described in \cref{sec:performance_measures}, the \gls{map} score is used to evaluate the performance of the neural network.
Since transfer learning is used and the dataset is fairly small, different hyperparameters could be tested.
As described in \cref{ssec:training}, grid search was used to find the best hyperparameters.
In \cref{tab:detection_eval} the \gls{map} scores for different hyperparameter combinations are listed.
LR is the learning rate and SR is the sample rate and determines the number of \gls{roi} proposals, as described in \cref{ssec:training}.\par
\begin{table}[htb]
    \centering
    \caption[Performance of the network for different hyperparameter]{Testing of different training hyperparameters and their influence on the \acrshort{ap} score.}
    \label{tab:detection_eval}
    \begin{tabular}{ccccccc}
        \toprule
        % \textbf{Epochs} & \textbf{LR} & \textbf{SR} & \textbf{Box mAP} & \textbf{Mask mAP} \\
        \multicolumn{3}{c}{\textbf{Parameter} } & \multicolumn{2}{c}{\textbf{Scores} }                                                      \\
        \cmidrule(lr){1-3}                       \cmidrule(lr){4-5}
        \textbf{Epochs}                         & \textbf{LR}                          & \textbf{SR} & \textbf{Box mAP} & \textbf{Mask mAP} \\
        \midrule
        1                                       & 0.001                                & 128         & 69.3             & 83.4              \\
        1                                       & 0.001                                & 512         & 84.8             & 85.7              \\
        1                                       & 0.003                                & 128         & 73.3             & 76.8              \\
        1                                       & 0.003                                & 512         & 83.1             & 89.6              \\
        3                                       & 0.001                                & 128         & 77.6             & 90.1              \\
        3                                       & 0.001                                & 512         & 87.3             & 90.8              \\
        3                                       & 0.003                                & 128         & 83.9             & 89.0              \\
        3                                       & 0.003                                & 512         & 84.9             & 87.6              \\
        \bottomrule
    \end{tabular}
\end{table}
Since the configuration with a learning rate of 0.001, sample rate of 512 and 3 epochs achieved the best results, all the following images and calculations were generated using this configuration.
A visualization of the prediction for some example frames when driving straight and from an angle at a ramp can be seen in \cref{fig:camera_prediction_straight} and \cref{fig:camera_prediction_angled} respectively.
In all those cases the detection of the ramp was successful and seems to be fairly accurate.
The annotation inside the image describes how confident the network is about the detection.
All images used for the training and evaluation are colored, but are visualized as grayscale image in this case, to allow for an easier distinction between the segmented region and the background.

\begin{figure}[htb]
    \centering
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{camera_prediction_straight}
        \caption{Driving straight at the ramp.}
        \label{fig:camera_prediction_straight}
    \end{subfigure}

    % \bigskip
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{camera_prediction_angled}
        \caption{Driving at an angle to the ramp.}
        \label{fig:camera_prediction_angled}
    \end{subfigure}
    \caption[Visualization examples of predicted segmentation masks]{Some examples of the predicted bounding box and mask when driving at a ramp. The class of the object and the confidence score are annotated as well. All those images were part of the validation set.}
\end{figure}

\subsection{Point Cloud Extraction}
As described in \cref{ssec:point_cloud_extraction}, the predicted segmentation mask can be used to only extract the points that are part of the ramp from the point cloud.
Different ramp properties can then be calculated from the point cloud.
In a first step, the points must be projected from 3D space into 2D space.
In \cref{fig:points_projection_mask_zed} the projected point cloud of the ZED 2i camera are shown as orange points.
The green points symbolize the points that were detected by the neural network as part of the ramp.
The blue box is the corresponding bounding box.
It can be seen that detection works very well and also only the actual drivable part of the ramp is marked, unlike when just the \gls{lidar} is used (see \cref{fig:points_projection}).
While most of the area is covered by the point cloud, objects further away than \SI{20}{\metre} are not detected.
This can be seen in the area above the ramp, which is too far away for the sensor to detect.
But since the car is not far away from the ramp, most of the ramp could be covered by points.\par
This problem is not present, when the point cloud of the \gls{lidar} is used instead.
The projection is shown in \cref{fig:points_projection_mask_lidar}.
It can be seen that the vertical resolution is worse at most opening angles, but the horizontal resolution is better.
This is especially apparent at the beginning of the ramp.
The neural network marked the pixels where the bounding box ends as part of the ramp, but because the \gls{lidar} does not have any points near this line, the nearest marked points are multiple \si{\cm} further away.
\isug{Maybe add specific range}
This leads to a wrong distance and perhaps length estimation.\par
Projecting the point cloud back into 3D space allows for the calculation of the properties of the ramp.
The results for both sensors are shown in \cref{tab:cloud_extraction_estimation}.
The box method uses the bounding box to extract the points from the point cloud and the mask method uses the mask to extract the points from the point cloud.
The \gls{ransac} algorithm has been applied in both cases to the extracted points, to remove outliers.
The used sensor is written in the subscript of the method name.
Since in the neural network is trained to only detect the drivable part of the ramp, the width had to be measured again.
For the ramp A, the width between the curbs was measured as \SI{2.79}{\metre}.\par
It can be seen that the point cloud generated by the ZED camera produces worse results than the point cloud generated by the lidar.
While it can not be seen very well in \cref{fig:points_projection_mask_zed}, the point cloud was very inaccurate.
The points on the ramp do not build a planar surface, that is why the \gls{ransac} algorithm had trouble to find an optimal plane.
The \gls{lidar} on the other hand produced an accurate point cloud and achieved good results.

\begin{figure}[htb]
    \centering
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{points_projection_mask_zed.pdf}
        \caption{Projection using the by the ZED camera generated point cloud.}
        \label{fig:points_projection_mask_zed}
    \end{subfigure}

    % \bigskip
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{points_projection_mask_lidar.pdf}
        \caption{Projection using the by the \acrshort{lidar} generated point cloud.}
        \label{fig:points_projection_mask_lidar}
    \end{subfigure}
    \caption[Point cloud extraction using the predicted segmentation mask]{Predicted bounding ion of the 3D point cloud onto a 2D image. Using the predicted segmentation mask, the points that lie in the ramp regions can now be extracted from the point cloud.}
\end{figure}
\begin{table}[H]
    \centering
    \caption[Estimation of ramp properties using the extracted point cloud]{Estimation of different ramp properties using the extracted point cloud.}
    \label{tab:cloud_extraction_estimation}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lSSSSSS}
            \toprule
            \textbf{Method}            & {\textbf{Angle} [\si{\degree}]} & {\textbf{Error} [\si{\degree}]} & {\textbf{Width} [\si{\metre}]} & {\textbf{Error} [\si{\metre}]} & {\textbf{Length} [\si{\metre}]} & {\textbf{Error} [\si{\metre}]} \\
            \midrule
            $\text{Box}_\text{Zed}$    & 8.84                            & 1.64                            & 3.74                           & 0.95                           & 13.18                           & 1.21                           \\
            $\text{Mask}_\text{Zed}$   & 8.72                            & 1.52                            & 2.53                           & 0.26                           & 12.61                           & 0.64                           \\
            $\text{Box}_\text{LiDAR}$  & 7.78                            & 0.58                            & 5.10                           & 2.31                           & 10.98                           & 0.99                           \\
            $\text{Mask}_\text{LiDAR}$ & 7.64                            & 0.44                            & 2.82                           & 0.03                           & 10.19                           & 1.78                           \\
            \bottomrule
        \end{tabular}
    }
\end{table}